{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f2e304",
   "metadata": {},
   "source": [
    "# Python Web-scrapping with BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a99e9d",
   "metadata": {},
   "source": [
    "## Description:\n",
    "\n",
    "   In this project, we will learn how to scrape a static website. We wish to extract the names of all the artists beginning with the letter \"Z\", their nationality,birth info and link to their bio from the [National Gallery of Art Website](https://web.archive.org/web/20121007172955/https://www.nga.gov/collection/anZ1.htm \"Artist Names Website\")\n",
    "\n",
    "   The website has four pages so we will use the for loop to iterate through all the pages. We will use the Python's BeautifulSoup library to scrape the website for info. \n",
    "   \n",
    "## Objectives\n",
    "- [x] [Scrape Artist Information](#Scrapping-Artist-Information)\n",
    "- [x] [Write output data into a CSV file](#Writing-a-CSV-file)\n",
    "- [x] [Read CSV into a Pandas DataFrame](#Reading-CSV-file-into-a-pandas-DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f70ab1",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52a5a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import chardet\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71ac246",
   "metadata": {},
   "source": [
    "### Scrapping Artist Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32769fba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pages=[]\n",
    "tup = []\n",
    "\n",
    "for i in range(1,5):\n",
    "    url=\"https://web.archive.org/web/20121007172955/https://www.nga.gov/collection/anZ\"+ str(i) +'.htm'\n",
    "    pages.append(url)\n",
    "for items in pages:\n",
    "    page = requests.get(items)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    last_links = soup.find(class_=\"AlphaNav\")\n",
    "    last_links.decompose()\n",
    "    \n",
    "    \n",
    "    namelist=soup.find(class_=\"BodyText\")\n",
    "    namelist_items=namelist.find_all(valign = 'top')\n",
    "    \n",
    "    for i in namelist_items:\n",
    "        name = i.find('a').text.replace(\",\", \" \")\n",
    "        links = 'https://web.archive.org'+ str(i.find('a').get('href'))\n",
    "        nationality= i.find('td').find_next_sibling('td').text.split(\",\")[0]\n",
    "        age =i.find('td').find_next_sibling('td').text.split(\",\")[-1]\n",
    "        tup.append((name,nationality,age,links))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e99ed",
   "metadata": {},
   "source": [
    "**Code Review 1:**\n",
    "```Python\n",
    "\n",
    "for i in range(1,5):\n",
    "    url=\"https://web.archive.org/web/20121007172955/https://www.nga.gov/collection/anZ\"+ str(i) +'.htm'\n",
    "    pages.append(url)\n",
    "```\n",
    "\n",
    "The Artist names' website has 4 pages in total. We use the above code to perform more iteration and the results are appended in the initialized list `pages=[]`.\n",
    "\n",
    "\n",
    "Now that we have a list of all the web pages, we use the `requests.get(Item)` method to collect each respective URL and assign it to the variable page.\n",
    "\n",
    "Next, we will create a BeautifulSoup object for each page. The object takes the arguments, `page.text` and `html.parser`. **Page.text** takes the content of the server's response and parses it from the in-bulit python library **html5lib**.\n",
    "\n",
    "\n",
    "`last_links = soup.find(class_=\"AlphaNav\")`\n",
    "\n",
    "`last_links.decompose()`\n",
    "    \n",
    "\n",
    "The above code will do away with the additional metadata tags contained on the website. Our desired info is conatined in the `<Div>` tag `class_=\"BodyText\"`. we will use another for loop to iterate through all the `<div>` tags and get the **Name, Nationality, Age Info and Bio link** of each artist and append them to the list initialized as `tup=[]`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ec18c",
   "metadata": {},
   "source": [
    "### Writing a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "164d66ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "filename = 'Z_Artist_names.csv'\n",
    "with open(filename, 'w', newline='') as f: \n",
    "    csvwriter=csv.writer(f)\n",
    "    hw=csv.DictWriter(f, fieldnames= ['Name','Nationality','Age_info','Bio_link'])\n",
    "    hw.writeheader()\n",
    "    for row in tup:\n",
    "        csvwriter.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79a720",
   "metadata": {},
   "source": [
    "**Code Review 2**\n",
    "\n",
    "Remember the List of tuples we initialized above as `tup=[]`? \n",
    "Now we wish to create a CSV file from the list of tuples and add a header. To initialize the header, we use the ***DictWriter()*** method and pass our desired column names as a list in the *fieldnames* attribute. \n",
    "\n",
    "We then use the ***writeheader()*** method to invoke the csvwriter object. Now that we have our header, we use the ***writerow()*** method from the ***write*** class to write data row-wise to a CSV file. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c1a584",
   "metadata": {},
   "source": [
    "### Reading CSV file into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f44fda4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "with open(filename, 'rb') as file:\n",
    "    print(chardet.detect(file.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "283d8b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nationality\n",
       "American         32\n",
       "Italian          29\n",
       "German           14\n",
       "Dutch             5\n",
       "Polish            4\n",
       "Czech             4\n",
       "French            3\n",
       "Spanish           3\n",
       "Flemish           2\n",
       "Russian           2\n",
       "Mexican           2\n",
       "Swiss             2\n",
       "British           2\n",
       "Swedish           2\n",
       "Chilean           1\n",
       "born 1943         1\n",
       "Yugoslavian       1\n",
       "Austrian          1\n",
       "Belgian           1\n",
       "Netherlandish     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('Z_Artist_names.csv',encoding='ISO-8859-1')\n",
    "df['Nationality'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd3b1b",
   "metadata": {},
   "source": [
    "When converting a CSV file into a DataFrame, one may encounter the **UnicodeDecodeError**.\n",
    "\n",
    "To avoid this error, use the ***chardet*** python library to detect the type of encoding used and pass the encoding type as an attribute when reading the CSV to a pandas dataframe. For my case the chardet output was as follows:\n",
    "\n",
    "***{'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}***\n",
    "\n",
    "The code below will convert your CSV file into a dataframe successfully. \n",
    "\n",
    "`df= pd.read_csv('Z_Artist_names.csv',encoding='ISO-8859-1')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940ced5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
